---
name: nitro-go-tests
on:
  workflow_call:
    inputs:
      run-name:
        required: true
        type: string
        description: "The name of the test run. It will be used for labeling artifacts."
      run-defaults-a:
        required: false
        type: boolean
      run-defaults-b:
        required: false
        type: boolean
      run-flaky:
        required: false
        type: boolean
      run-pathdb:
        required: false
        type: boolean
      run-stylus:
        required: false
        type: boolean
      run-challenge:
        required: false
        type: boolean
      run-l3challenge:
        required: false
        type: boolean
      run-legacy-challenge:
        required: false
        type: boolean
      run-race:
        required: false
        type: boolean
      run-long:
        required: false
        type: boolean
      run-execution-spec-tests:
        required: false
        type: boolean
      run-pebble:
        required: false
        type: boolean

jobs:
  go-tests:
    name: Go tests
    runs-on: arbitrator-ci
    services:
      redis:
        image: redis
        ports:
          - 6379:6379

    steps:
      - name: Checkout
        uses: actions/checkout@v6
        with:
          submodules: recursive

      - name: Setup CI
        uses: ./.github/actions/ci-setup

      - name: Set environment variables
        run: |
          mkdir -p target/tmp/deadbeefbee
          echo "TMPDIR=$(pwd)/target/tmp/deadbeefbee" >> "$GITHUB_ENV"
          echo "GOMEMLIMIT=6GiB" >> "$GITHUB_ENV"
          echo "GOGC=80" >> "$GITHUB_ENV"
          echo "GITHUB_TOKEN=${{ secrets.GITHUB_TOKEN }}" >> "$GITHUB_ENV"

      - name: Build
        run: make -j8 build test-go-deps

      - name: Build node dependencies
        run: make -j8 build-node-deps

      # --------------------- PATHDB MODE ---------------------

      - name: run tests without race detection and path state scheme
        if: inputs.run-pathdb
        run: >-
          ${{ github.workspace }}/.github/workflows/gotestsum.sh
          --tags cionly --timeout 90m --cover --test_state_scheme path

      # --------------------- DEFAULTS MODE ---------------------

      - name: run tests without race detection and hash state scheme (A-batch)
        if: inputs.run-defaults-a
        id: run-tests-defaults-a
        continue-on-error: true
        run: >-
          ${{ github.workspace }}/.github/workflows/gotestsum.sh
          --tags cionly --timeout 60m --test_state_scheme hash
          --junitfile test-results/junit-a.xml --run '^Test[A-L]'

      - name: run tests without race detection and hash state scheme (B-batch)
        if: inputs.run-defaults-b
        id: run-tests-defaults-b
        continue-on-error: true
        run: >-
          ${{ github.workspace }}/.github/workflows/gotestsum.sh
          --tags cionly --timeout 60m --test_state_scheme hash
          --junitfile test-results/junit-b.xml --skip '^Test[A-L]'

      - name: Process JUnit XML logs
        if: (inputs.run-defaults-a || inputs.run-defaults-b) && always()
        run: python3 ${{ github.workspace }}/.github/workflows/process_junit.py test-results/

      - name: Upload Go test Artifacts
        if: (inputs.run-defaults-a || inputs.run-defaults-b) && always()
        uses: actions/upload-artifact@v5
        with:
          name: junit-reports-go-${{ inputs.run-defaults-a && 'defaults-a' || 'defaults-b' }}
          path: test-results/junit*.xml

      - name: Fail if tests failed
        if: (inputs.run-defaults-a && steps.run-tests-defaults-a.outcome == 'failure') || (inputs.run-defaults-b && steps.run-tests-defaults-b.outcome == 'failure')
        run: |
          echo "One or more tests failed."
          exit 1

      - name: run redis tests
        if: inputs.run-defaults-a
        run: >-
          gotestsum --format short-verbose -- -p 1 -run TestRedis ./arbnode/... ./system_tests/...
          -coverprofile=coverage-redis.txt -covermode=atomic -coverpkg=./... -- --test_redis=redis://localhost:6379/0

      - name: Upload coverage to Codecov
        if: inputs.run-defaults-a || inputs.run-defaults-b
        uses: codecov/codecov-action@v5
        with:
          fail_ci_if_error: false
          files: ./coverage.txt,./coverage-redis.txt
          verbose: false
          token: ${{ secrets.CODECOV_TOKEN }}

      - name: create block input json file
        if: inputs.run-defaults-a
        run: >-
          gotestsum --format short-verbose -- -run TestProgramStorage$ ./system_tests/... --count 1 --
          --recordBlockInputs.enable=true --recordBlockInputs.WithBaseDir="${{ github.workspace }}/target"
          --recordBlockInputs.WithTimestampDirEnabled=false --recordBlockInputs.WithBlockIdInFileNameEnabled=false

      - name: run arbitrator prover on block input json
        if: inputs.run-defaults-a
        run: |
          make build-prover-bin
          target/bin/prover target/machines/latest/machine.wavm.br -b \
          --json-inputs="${{ github.workspace }}/target/TestProgramStorage/block_inputs.json"

      - name: run jit prover on block input json
        if: inputs.run-defaults-a
        run: |
          make build-jit
          if [ -n "$(target/bin/jit \
              --binary target/machines/latest/replay.wasm \
              --cranelift \
              --json-inputs='${{ github.workspace }}/target/TestProgramStorage/block_inputs.json')" ]; then
            echo "Error: Command produced output."
            exit 1
          fi

      # --------------------- FLAKY MODE --------------------------

      - name: run flaky tests
        if: inputs.run-flaky
        continue-on-error: true
        run: >-
          ${{ github.workspace }}/.github/workflows/gotestsum.sh
          --tags cionly --timeout 60m --test_state_scheme hash --flaky

      # --------------------- CHALLENGE MODES ---------------------

      - name: build challenge tests
        if: inputs.run-challenge
        run: >-
          ${{ github.workspace }}/.github/workflows/gotestsum.sh --tags challengetest
          --run TestChallenge --timeout 120m --cover

      - name: run L3 challenge tests
        if: inputs.run-l3challenge
        run: >-
          ${{ github.workspace }}/.github/workflows/gotestsum.sh
          --tags challengetest --run TestL3Challenge --timeout 120m --cover

      - name: run legacy challenge tests
        if: inputs.run-legacy-challenge
        run: >-
          ${{ github.workspace }}/.github/workflows/gotestsum.sh --tags legacychallengetest
          --run TestChallenge --timeout 60m --cover

      # --------------------- STYLUS MODE ---------------------

      - name: run stylus tests
        if: inputs.run-stylus
        run: >-
          ${{ github.workspace }}/.github/workflows/gotestsum.sh
          --tags stylustest --run TestProgramArbitrator --timeout 60m --cover

      # --------------------- RACE MODE ---------------------

      - name: run race tests
        if: inputs.run-race
        run: >-
          ${{ github.workspace }}/.github/workflows/gotestsum.sh 
          --race --timeout 90m --test_state_scheme hash


      # --------------------- LONG MODE ---------------------

      - name: run long tests
        if: inputs.run-long
        run: >-
          ${{ github.workspace }}/.github/workflows/gotestsum.sh --tags stylustest
          --run TestProgramLong --timeout 60m --cover

      # --------------------- EXECUTION SPEC MODE ---------------------

      - name: run execution spec tests
        if: inputs.run-execution-spec-tests
        run: ${{ github.workspace }}/.github/workflows/runExecutionSpecTests.sh

      # --------------------- PEBBLE MODE ---------------------

      - name: run pebble tests
        if: inputs.run-pebble
        run: ${{ github.workspace }}/.github/workflows/gotestsum.sh --timeout 90m --test_database_engine pebble

      # --------------------- ARCHIVE LOGS FOR ALL MODES ---------------------

      - name: Archive detailed run log
        uses: actions/upload-artifact@v5
        with:
          name: ${{ inputs.run-name }}-full.log
          path: full.log
